\section{Metodologia}\label{sec:metodologia}

Neste projeto foram comparadas técnicas para o ajuste de hiperparâmetros de algoritmos de classificação em dinâmica da digitação. Para isso, foram utilizados conjuntos de dados disponíveis publicamente, conforme descrito na Subseção~\ref{subsec:datasets}. Esses dados foram divididos entre treino e teste, sendo que as amostras usadas para treinamento serão referentes a dados mais antigos em comparação com os dados usados para teste. Na Seção~\ref{sec:metricas}, são descritas métricas que serão usadas para avaliação de desempenho neste trabalho.


\subsection{Conjuntos de dados}\label{subsec:datasets}

Grande parte dos trabalhos que realizaram experimentos com dados de dinâmica da digitação não disponibilizaram os dados coletados~\cite{Roy2022systematic}. Esse fato dificulta a reprodutibilidade de estudos na área. Neste projeto foram utilizados dados publicamente disponíveis. Os conjuntos de dados usados são descritos a seguir:


\subsubsection{CMU}\label{subsubsec:cmu}

Este conjunto de dados~\footnote{\url{https://www.cs.cmu.edu/~keystroke/}} possui dados de 51 indivíduos que digitaram a senha ``.tie5Roanl'' em oito sessões de captura, com 50 amostras em cada sessão. No total, cada indivíduo digitou a senha 400 vezes. É importante destacar que, assim como em outros conjuntos de dados públicos e privados, houve um intervalo de tempo entre as sessões de coleta, para que as variações na digitação de cada usuário fossem consideradas no modelo. Cada pessoa participou de somente uma sessão por dia.

Os autores explicam que dentre os motivos da senha ser fixa para todos os usuários estão o viés que poderia surgir nos experimento se cada usuário pudesse escolher a própria senha e a necessidade de coletar amostras de impostores específicas para cada usuário, o que tornaria a coleta de dados ainda mais difícil.


\subsubsection{KeyRecs}\label{subsubsec:keyrecs}

O conjunto de dados KeyRecs~\footnote{\url{https://zenodo.org/records/7886743}} envolveu a captura de dinâmica da digitação de texto fixo e de texto livre. Como o foco deste projeto foi em texto fixo, apenas essa parte do conjunto de dados foi utilizada. Para texto fixo, de acordo com a descrição do conjunto de dados, 99 indivíduos digitaram uma mesma senha em duas sessões, com 100 amostras em cada sessão, totalizando 200 amostras por indivíduo. Ao realizar o download da versão disponível, entretanto, observou-se que alguns usuários tem menos do que 200 amostras.


\subsection{Métricas}\label{sec:metricas}

Esta seção descreve algumas métricas usadas na literatura que serão usadas para avaliação dos resultados nos experimentos realizados neste projeto de pesquisa. Essas métricas são: FMR, FNMR e acurácia balanceada~\cite{Precise2014, Ferlini2021eargate}. Uma breve descrição dessas métricas é apresentadas a seguir:

\begin{itemize}
    \item FMR (\textit{False Match Rate}, Taxa de falsa correspondência): percentual de tentativas de impostores que foram aceitas como genuínas, definida como

          \begin{equation}\label{eqn:fmr}
              FMR = \frac{numero\:de\:tentativas\:de\:impostores\:aceitas}{total\:de\:tentativas\:de\:impostores}.
          \end{equation}

          Uma taxa relacionada é a FAR (\textit{False Acceptance Rate}), que tem significado similar, mas considera também taxa em que o sistema biométrico falha ao obter uma amostra biométrica. Essa taxa é conhecida como FTA (\textit{Failure to Acquire Rate}).


    \item FNMR (\textit{False Non-match Rate}, Taxa de falsa não-correspondência): percentual de tentativas genuínas que foram rejeitadas como impostoras pelo sistema, definida como

          \begin{equation}\label{eqn:fnmr}
              FNMR = \frac{numero\:de\:tentativas\:genuinas\:rejeitadas}{total\:de\:tentativas\:de\:usuarios\:genuinos}.
          \end{equation}

          Uma métrica relacionada é a FRR (\textit{False Rejection Rate}), que tem um significado similar, mas considera também a FTA.

    \item Acurácia balanceada: média do acerto para cada classe (genuíno e impostor). Essa métrica pode ser obtida a partir da o cálculo da (HTER -~\textit{Half Total Error}, Metade do erro total)

          \begin{equation}\label{eqn:hter}
              HTER = \frac{FNMR + FMR}{2},
          \end{equation}

          definida como a média entre FNMR e FMR~\cite{Roy2022systematic}. A partir da HTER, então é obtida a acurácia balanceada, definida como

          \begin{equation}\label{eqn:bacc}
              BAcc = 1 - HTER.
          \end{equation}

\end{itemize}


\subsection{Divisão de dados de treino, validação e teste}

A divisão de dados entre treino e teste foi feita com base no momento em que as amostras de digitação foram coletadas. Em ambos os conjuntos de dados foram utilizadas as primeiras 50 amostras de cada um dos usuários para treinamento, o que corresponde à primeira sessão de coleta do CMU e na metade dos registros da primeira sessão de coleta do Keyrecs. O restante dos dados de cada usuário foi utilizado para testes. No CMU, isso corresponde às 7 sessões que sucedem a primeira. No Keyrecs, isso correspondem à segunda metade das amostras da primeira sessão e todas as amostras da segunda sessão.   

Em alguns algoritmos de Aprendizado de Máquina, somente os dados do usuário genuíno são utilizados para treinamento, e o modelo é então testado com dados de usuários impostores e de usuários genuínos. Estes modelos são chamados de~\textit{detectores de anomalias}~\cite{Killourhy2009}. Para outros algoritmos, ambos os dados de usuários impostores e genuínos devem ser utilizados para treinamento. Nesse sentido, é importante que a quantidade de amostras para cada classe (impostor e genuíno) sejam as mesmas, para que o modelo criado não seja enviesado. Dessa forma, foram selecionadas aleatoriamente 50 amostras de usuários impostores para treinamento, sendo que no CMU foi considerada apenas a primeira sessão de cada usuário impostor e no Keyrecs somente as primeiras 50 amostras para seleção.

Para o ajuste de hiperparâmetros foi feita uma validação cruzada sobre as amostras de treinamento, em que 4/5 do total de amostras foram utilizadas para a criação de um modelo com uma configuração de hiperparâmetros específica e 1/5 foi utilizada para testar o modelo. A média da $BAcc$ obtida em cada divisão foi considerada para selecionar a melhor configuração. Nos casos em que foi necessário utilizar dados de impostores para treinamento, os conjuntos de treinamento com amostras de usuários genuínos e impostores foram divididos separadamente e 4/5 de ambos os conjuntos foi utilizado para treinamento e 1/5 para testes.


\subsection{Algorítmos e Hiperparâmetros}

Foram utilizados três algoritmos de Aprendizado de Máquina neste trabalho, sendo eles o~\textit{Random Forest} (RF), a~\textit{Support Vector Machine} (SVM) e o algoritmo desenvolvido por~\citeonline{MagalhaesStatistical} (ST). Eles foram usados para criar modelos de verificação para cada usuário com base em suas amostras de digitação nos dois conjuntos de dados mencionados anteriormente. Com cada algoritmo foram avaliadas 3 abordagens:

\begin{itemize}
    \item Sem ajuste de hiperparâmetros: os modelos foram criados com a configuração padrão de hiperparâmetros da biblioteca utilizada (Scikit-learn);
    \item Ajuste de hiperparâmetros por usuário: para cada usuário foram testadas todas as configurações possíveis de hiperparâmetros e foi selecionada a que gerou um modelo com maior BAcc;
    \item Ajuste de hiperparâmetros global: cada configuração de hiperparâmetros foi utilizada para a criação de um modelo para cada usuário e foi então calculada a BAcc média dos modelos. Foi selecionada a configuração que gerou modelos cuja a BAcc média foi a mais alta. 
\end{itemize}

Em cada uma das abordagens a melhor configuração de hiperparâmetros encontrada foi utilizada para o treinamento dos modelos para cada usuário, com todo o conjunto de dados para treinamento.
